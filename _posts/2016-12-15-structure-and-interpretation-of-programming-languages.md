---
title:  "Structure and Interpretation of Programming Languages"
date:   2016-12-15
author: paxorus
---

Structure and Interpretation of Programming Languages

MIT’s 6.001, “Structure and Interpretation of Computer Programs,” was first taught in 1980 by Hal Abelson and Gerry Sussman. Since then, the course has spread to other universities such as Berkeley and Brandeis. In 2008 MIT discontinued the course. According to [this article](http://www.posteriorscience.net/?p=206) it’s not relevant in today’s world of programming by poking, rather than understanding the underlying components. [This article](http://www.niemanlab.org/2011/11/in-praise-of-impractical-programming/), praises the course but as an impractical but fun detour. I’ve also heard students call the course useless and terrifying.

In the 21st century ecosystem, where new languages and frameworks are constantly coming in and being stacked on top of existing ones, I beg to differ. We don’t create programming languages by designing syntax; we create it by implementing an evaluator, or interpreter, for it. The syntax, along with features such as garbage collection, memoization, inheritance, and evaluation patterns (memoization, laziness), is superficial fluff. If I define a language where “h” runs a solution to the Halting Problem but I can’t write an evaluator for it, have I really created that language? If Hindi can be spoken and not understood, would we really call it a language, or just gibberish? So creating a new language or framework isn’t about declaring that this will do that, or that will do this. It’s about writing the code to actually do it. Just defining the syntax is as good as waving your hands in the air.

That means knowing how to write a compiler (language-to-language converter) or an evaluator (language-to-action converter) is increasingly relevant today. Abstractions are all the rage now, where every programmer dreams of creating their own language. Creating those abstractions will require writing a new compiler or evaluator.

The course teaches a few essential ideas of interpretation which I might revisit, but one plus of Lisp and several of its dialects (e.g., Common Lisp, Scheme) is that they largely obviate the mundane parsing step. There’s no need to create a parse tree, as the structure of every Scheme and Lisp program already IS a tree. In addition, the choice of Lisp allows the course to cover functional programming in great depth, as well as several language features such as tail recursion, memoization, infinite data structures, and first-class functions.

Recursion gives us an intuitive way of coding a recurrence relation. If the closed-form expression is less intuitive, keep it open-form! Divide-and-conquer algorithms, trees, and streams are very common examples of such relations. Recursion is non-essential as long as the language can push data to a stack, a simulation of all recursion really is, but it is something that’s largely out-of-the-box when the language already maintains a call stack. So whether you take recursion for granted is up to you, but a theoretical side-note is that there’s always a closed-form way to express any code that uses recursion. A problem never “needs” recursion.

Because the size of the call stacks can grow wildly, an important language feature is tail recursion, an optimization where a function call isn’t kept in the stack if it made a recursive call as its last action. The next function call was its last action, so there’s no need to keep it around in memory.

The class also covers memoization (not memorization). This feature is a simple optimization that trades memory for time. The first time a memoized function encounters some input, it will store the input-output pair in a lookup table. On subsequent encounters, it will consult the lookup table and return the output without re-computing the result. Any key-value store like a list of tuples or a hash map will do.

Infinite data structures are gifts of lazy evaluation. An infinite data structure can’t be fully held in memory, so a few values are held in memory along with rules (often functions) to define the next value or values. Their real-world application is generally for modeling finite but incredibly large data. Streams, available in Java 8, and very easy to implement in Scheme, are common in big data applications.

Currying is an interesting aside, and not altogether impractical. Since functions are first-class in Lisp (they can be variables, arguments, and return values), a function can be curried by supplying a subset of its parameters. The idea is to define a new function that accepts some parameters and defines the rest itself. In its body it immediately applies them to the original function. Haskell makes extensive use of this (the practice of currying is named after Haskell Curry). To add two numbers, you would call the function with the first number, which would return a second function which you would call with the second number, yielding the sum.

This, and partial function application in general, are important in security. If you want to keep a stack private to your code but share the ability to push items, you can create a function accepts an item and pushes it to the stack which is in its closure. You can provide functions as “capabilities” to client code which never gets direct access to the original variables. It’s a solution to a common problem I’ve faced in production code, primarily Java, where core objects are exposed to the client layer and the client is able to call more methods on those objects than we would like. Until Java 8’s lambda expressions, the only solution was a repugnant one, to write an entire class or anonymous class that would sit between the library and the client. In JavaScript production code, with first-class functions it’s been a cinch every time.

Of course, the class can’t cover every language feature in existence, and Lisp dialects aren’t the right choice for something like object-oriented programming or garbage collection, but it sets the student on a path to think about these things the next time they write a library or API, or even a function. The student can make educated decisions about performance optimization and implementing expressive yet simple syntax. And these takeaways don’t count the invaluable functional paradigm that’s also taught in the class (see my next post).

Useless, impractical, and irrelevant are absolutely not words I would use to describe the structure and interpretation of computer programs. Terrifying? The parentheses and lambda calculus can get to you.
